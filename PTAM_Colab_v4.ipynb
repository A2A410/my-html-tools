{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶ PTAM Colab Compiler v4\n",
        "\n",
        "**Plain Text Archive Merger + Token Dictionary Builder**\n",
        "\n",
        "**Performance-Tuned Edition**\n",
        "\n",
        "Process ZIPs and files ‚Üí Generate PTAM + Progressive Dictionary\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üìÅ Setup Environment & Dependencies\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"Setting up PTAM environment...\\n\")\n",
        "\n",
        "# Install py7zr for 7z support\n",
        "print(\"üì¶ Installing py7zr...\")\n",
        "!pip install -q py7zr\n",
        "print(\"‚úì py7zr installed\\n\")\n",
        "\n",
        "# Create working directories\n",
        "print(\"Creating working directories...\\n\")\n",
        "dirs = [\n",
        "    \"/content/DataOfEverything\",\n",
        "    \"/content/SpecialHubAccess\",\n",
        "    \"/content/UUID\"\n",
        "]\n",
        "\n",
        "for directory in dirs:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    print(f\"‚úì {directory}\")\n",
        "\n",
        "print(\"\\n‚úÖ Environment ready!\")\n",
        "print(\"\\nSupported: ZIP, TAR, GZIP, BZIP2, XZ, 7Z\")\n",
        "print(\"Nested archives: Up to 5 levels\")"
      ],
      "metadata": {
        "id": "setup",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üéØ PTAM Configuration\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ‚öôÔ∏è Processing Mode\n",
        "PTAM_MODE = \"Plain\" #@param [\"Plain\", \"Token\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üìÅ Working Directory\n",
        "WORKING_DIR = \"/content/DataOfEverything\" #@param [\"/content/DataOfEverything\", \"/content/SpecialHubAccess\", \"/content/UUID\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üìù Output Filename\n",
        "OUTPUT_FILENAME = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ‚ö° Performance Settings (Tuned)\n",
        "\n",
        "#@markdown **Chunk Size** - How much data to process at once\n",
        "CHUNK_SIZE = \"Level 1 (512 KB)\" #@param [\"Level 1 (512 KB)\", \"Level 2 (1 MB)\", \"Level 3 (2 MB)\"]\n",
        "\n",
        "#@markdown **Batch Size** - Files per batch before cleanup\n",
        "BATCH_SIZE = \"Level 1 (50 files)\" #@param [\"Level 1 (50 files)\", \"Level 2 (100 files)\", \"Level 3 (250 files)\"]\n",
        "\n",
        "#@markdown **Process Count** - Multiprocessing workers\n",
        "PROCESS_COUNT = \"Level 1 (Single)\" #@param [\"Level 1 (Single)\", \"Level 2 (Dual)\"]\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# Parse settings\n",
        "CHUNK_MAP = {\n",
        "    \"Level 1 (512 KB)\": 524288,\n",
        "    \"Level 2 (1 MB)\": 1048576,\n",
        "    \"Level 3 (2 MB)\": 2097152\n",
        "}\n",
        "\n",
        "BATCH_MAP = {\n",
        "    \"Level 1 (50 files)\": 50,\n",
        "    \"Level 2 (100 files)\": 100,\n",
        "    \"Level 3 (250 files)\": 250\n",
        "}\n",
        "\n",
        "PROCESS_MAP = {\n",
        "    \"Level 1 (Single)\": 1,\n",
        "    \"Level 2 (Dual)\": 2\n",
        "}\n",
        "\n",
        "CHUNK_SIZE_BYTES = CHUNK_MAP[CHUNK_SIZE]\n",
        "BATCH_SIZE_NUM = BATCH_MAP[BATCH_SIZE]\n",
        "PROCESS_COUNT_NUM = PROCESS_MAP[PROCESS_COUNT]\n",
        "\n",
        "print(\"‚úì Configuration loaded\")\n",
        "print(f\"  Mode: {PTAM_MODE}\")\n",
        "print(f\"  Working Directory: {WORKING_DIR}\")\n",
        "print(f\"  Output: {OUTPUT_FILENAME if OUTPUT_FILENAME else '[auto-generated]'}\")\n",
        "print(f\"\\n‚ö° Performance:\")\n",
        "print(f\"  Chunk: {CHUNK_SIZE_BYTES:,} bytes\")\n",
        "print(f\"  Batch: {BATCH_SIZE_NUM} files\")\n",
        "print(f\"  Processes: {PROCESS_COUNT_NUM}\")"
      ],
      "metadata": {
        "id": "config",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂Ô∏è Run PTAM Compiler\n",
        "\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import tarfile\n",
        "import gzip\n",
        "import bz2\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import shutil\n",
        "import tempfile\n",
        "import py7zr\n",
        "import gc\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import time\n",
        "\n",
        "# Progress bar\n",
        "def print_progress(current, total, prefix='', width=50):\n",
        "    percent = 100 * (current / float(total))\n",
        "    filled = int(width * current // total)\n",
        "    bar = '‚ñà' * filled + '‚ñë' * (width - filled)\n",
        "    print(f'\\r{prefix} |{bar}| {percent:.1f}% ({current}/{total})', end='', flush=True)\n",
        "    if current == total:\n",
        "        print()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# CONFIGURATION\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n\" + \"‚ïê\" * 60)\n",
        "print(\"PTAM COMPILER v4 - PERFORMANCE TUNED\")\n",
        "print(\"‚ïê\" * 60)\n",
        "\n",
        "# Setup paths\n",
        "if WORKING_DIR == \"/content/UUID\":\n",
        "    import uuid\n",
        "    uuid_file = \"/content/.ptam_uuid\"\n",
        "    if os.path.exists(uuid_file):\n",
        "        with open(uuid_file) as f:\n",
        "            session_uuid = f.read().strip()\n",
        "    else:\n",
        "        session_uuid = str(uuid.uuid4())[:8]\n",
        "        with open(uuid_file, 'w') as f:\n",
        "            f.write(session_uuid)\n",
        "    WORKING_DIR = f\"/content/{session_uuid}\"\n",
        "    os.makedirs(WORKING_DIR, exist_ok=True)\n",
        "    print(f\"\\n[UUID] {session_uuid}\")\n",
        "\n",
        "OUTPUT_PATH_1 = os.path.join(WORKING_DIR, \"ptam_output\")\n",
        "OUTPUT_PATH_2 = os.path.join(WORKING_DIR, \"ptam_dictionaries\")\n",
        "os.makedirs(OUTPUT_PATH_1, exist_ok=True)\n",
        "os.makedirs(OUTPUT_PATH_2, exist_ok=True)\n",
        "\n",
        "if not OUTPUT_FILENAME or OUTPUT_FILENAME.strip() == \"\":\n",
        "    OUTPUT_FILENAME = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "PTAM_FILE = os.path.join(OUTPUT_PATH_1, f\"{OUTPUT_FILENAME}.ptam.txt\")\n",
        "DICT_FILE = os.path.join(OUTPUT_PATH_2, \"token_dictionary.json\")\n",
        "HEATMAP_FILE = os.path.join(OUTPUT_PATH_2, \"token_heatmap.json\")\n",
        "\n",
        "print(f\"\\n[CONFIG] Mode: {PTAM_MODE}\")\n",
        "print(f\"[CONFIG] Chunk: {CHUNK_SIZE_BYTES:,} bytes\")\n",
        "print(f\"[CONFIG] Batch: {BATCH_SIZE_NUM} files\")\n",
        "print(f\"[CONFIG] Processes: {PROCESS_COUNT_NUM}\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# CONSTANTS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "MEDIA_EXTS = {\n",
        "    'jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp', 'ico', 'svg',\n",
        "    'mp4', 'mp3', 'wav', 'avi', 'mov', 'mkv', 'flac', 'ogg',\n",
        "    'pdf', 'exe', 'dll', 'so', 'dylib', 'bin', 'dat',\n",
        "    'woff', 'woff2', 'ttf', 'eot', 'otf'\n",
        "}\n",
        "\n",
        "ARCHIVE_EXTS = {'zip', 'tar', 'gz', 'tgz', 'bz2', 'tbz2', '7z', 'xz'}\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# HELPER FUNCTIONS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def is_text_readable(content):\n",
        "    if not content:\n",
        "        return False\n",
        "    sample = content[:1000] if len(content) > 1000 else content\n",
        "    printable = sum(1 for c in sample if 32 <= ord(c) <= 126 or c in '\\n\\r\\t')\n",
        "    return (printable / len(sample)) >= 0.8\n",
        "\n",
        "def should_skip(filename):\n",
        "    ext = Path(filename).suffix.lower().lstrip('.')\n",
        "    return ext in MEDIA_EXTS\n",
        "\n",
        "def is_archive(filename):\n",
        "    ext = Path(filename).suffix.lower().lstrip('.')\n",
        "    if ext in ['gz', 'bz2', 'xz']:\n",
        "        if Path(filename).stem.endswith('.tar'):\n",
        "            return True\n",
        "    return ext in ARCHIVE_EXTS\n",
        "\n",
        "def extract_archive(archive_path, extract_to):\n",
        "    try:\n",
        "        ext = Path(archive_path).suffix.lower().lstrip('.')\n",
        "        \n",
        "        if ext == 'zip':\n",
        "            with zipfile.ZipFile(archive_path, 'r') as zf:\n",
        "                zf.extractall(extract_to)\n",
        "        elif ext == '7z':\n",
        "            with py7zr.SevenZipFile(archive_path, 'r') as szf:\n",
        "                szf.extractall(extract_to)\n",
        "        elif ext in ['tar', 'gz', 'bz2', 'xz', 'tgz', 'tbz2']:\n",
        "            mode = 'r'\n",
        "            if ext in ['gz', 'tgz']:\n",
        "                mode = 'r:gz'\n",
        "            elif ext in ['bz2', 'tbz2']:\n",
        "                mode = 'r:bz2'\n",
        "            elif ext == 'xz':\n",
        "                mode = 'r:xz'\n",
        "            with tarfile.open(archive_path, mode) as tf:\n",
        "                tf.extractall(extract_to)\n",
        "        \n",
        "        files = []\n",
        "        for root, _, filenames in os.walk(extract_to):\n",
        "            for f in filenames:\n",
        "                files.append(os.path.join(root, f))\n",
        "        return files\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "def process_archive(archive_path, base_name, depth=0, max_depth=5):\n",
        "    if depth >= max_depth:\n",
        "        return []\n",
        "    \n",
        "    contents = []\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    \n",
        "    try:\n",
        "        extracted = extract_archive(archive_path, temp_dir)\n",
        "        \n",
        "        for filepath in extracted:\n",
        "            rel_path = os.path.relpath(filepath, temp_dir)\n",
        "            full_path = f\"{base_name}/{rel_path}\"\n",
        "            \n",
        "            if is_archive(filepath):\n",
        "                nested = process_archive(filepath, full_path.rsplit('.', 1)[0], depth + 1)\n",
        "                contents.extend(nested)\n",
        "            elif not should_skip(filepath):\n",
        "                try:\n",
        "                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        content = f.read()\n",
        "                    if content.strip() and is_text_readable(content):\n",
        "                        contents.append({'path': full_path, 'content': content})\n",
        "                except:\n",
        "                    pass\n",
        "    finally:\n",
        "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "    \n",
        "    return contents\n",
        "\n",
        "def format_size(b):\n",
        "    for u in ['B', 'KB', 'MB', 'GB']:\n",
        "        if b < 1024:\n",
        "            return f\"{b:.1f}{u}\"\n",
        "        b /= 1024\n",
        "    return f\"{b:.1f}TB\"\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# FILE SCANNING\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n[SCAN] Discovering files...\")\n",
        "\n",
        "all_files = []\n",
        "archives = []\n",
        "\n",
        "for root, dirs, files in os.walk(WORKING_DIR):\n",
        "    if 'ptam_output' in root or 'ptam_dictionaries' in root:\n",
        "        continue\n",
        "    for f in files:\n",
        "        fp = os.path.join(root, f)\n",
        "        if is_archive(f):\n",
        "            archives.append(fp)\n",
        "        elif not should_skip(f):\n",
        "            all_files.append(fp)\n",
        "\n",
        "print(f\"[SCAN] Archives: {len(archives)}\")\n",
        "print(f\"[SCAN] Text files: {len(all_files)}\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# CONTENT EXTRACTION (BATCHED)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n[EXTRACT] Processing archives...\")\n",
        "\n",
        "contents = []\n",
        "stats = {'valid': 0, 'skip': 0, 'empty': 0}\n",
        "\n",
        "# Archives\n",
        "for idx, archive in enumerate(archives):\n",
        "    name = Path(archive).stem\n",
        "    extracted = process_archive(archive, name)\n",
        "    contents.extend(extracted)\n",
        "    stats['valid'] += len(extracted)\n",
        "    print_progress(idx + 1, len(archives), '[EXTRACT] Archives')\n",
        "\n",
        "# Text files in batches\n",
        "print(\"\\n[EXTRACT] Processing text files...\")\n",
        "\n",
        "for i in range(0, len(all_files), BATCH_SIZE_NUM):\n",
        "    batch = all_files[i:i + BATCH_SIZE_NUM]\n",
        "    \n",
        "    for fp in batch:\n",
        "        try:\n",
        "            with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read()\n",
        "            if content.strip() and is_text_readable(content):\n",
        "                rel = os.path.relpath(fp, WORKING_DIR)\n",
        "                contents.append({'path': rel, 'content': content})\n",
        "                stats['valid'] += 1\n",
        "            else:\n",
        "                stats['empty'] += 1\n",
        "        except:\n",
        "            stats['skip'] += 1\n",
        "    \n",
        "    print_progress(min(i + BATCH_SIZE_NUM, len(all_files)), len(all_files), '[EXTRACT] Text files')\n",
        "    gc.collect()  # Cleanup after each batch\n",
        "\n",
        "print(f\"\\n[EXTRACT] Valid: {stats['valid']} | Skip: {stats['skip']} | Empty: {stats['empty']}\")\n",
        "\n",
        "if stats['valid'] == 0:\n",
        "    print(\"\\n[ERROR] No content to process\")\n",
        "    raise SystemExit()\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# DICTIONARY CREATION (TOKEN MODE ONLY - NO REFINEMENT)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "token_dict = {}\n",
        "heatmap = {}\n",
        "use_tokens = False\n",
        "\n",
        "# Always check for existing\n",
        "print(\"\\n[DICT] Checking for existing dictionary...\")\n",
        "\n",
        "if os.path.exists(DICT_FILE):\n",
        "    with open(DICT_FILE) as f:\n",
        "        token_dict = json.load(f)\n",
        "    print(f\"[DICT] ‚úì Loaded {len(token_dict)} tokens\")\n",
        "    use_tokens = True\n",
        "else:\n",
        "    print(f\"[DICT] No existing dictionary\")\n",
        "\n",
        "# Build NEW dictionary ONLY if Token mode AND no existing dict\n",
        "if PTAM_MODE == \"Token\" and not use_tokens:\n",
        "    print(f\"\\n[DICT] Building NEW dictionary (one-time, no refinement)...\")\n",
        "    \n",
        "    patterns = [\n",
        "        r'https?://[^\\s]+',\n",
        "        r'[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
        "        r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b',\n",
        "        r'[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}',\n",
        "        r'\\b0x[a-fA-F0-9]+\\b',\n",
        "        r'\\b[a-zA-Z_][a-zA-Z0-9_]{5,}\\b',\n",
        "        r'\\b\\d{6,}\\.?\\d*\\b',\n",
        "    ]\n",
        "    \n",
        "    # Batch accumulation\n",
        "    all_candidates = Counter()\n",
        "    \n",
        "    for idx, item in enumerate(contents):\n",
        "        local_counts = Counter()\n",
        "        for pattern in patterns:\n",
        "            for match in re.findall(pattern, item['content']):\n",
        "                if len(match) >= 6:\n",
        "                    local_counts[match] += 1\n",
        "        \n",
        "        # Merge locally accumulated counts\n",
        "        all_candidates.update(local_counts)\n",
        "        \n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print_progress(idx + 1, len(contents), '[DICT] Extracting')\n",
        "    \n",
        "    print_progress(len(contents), len(contents), '[DICT] Extracting')\n",
        "    \n",
        "    # Build dictionary from candidates\n",
        "    heatmap = dict(all_candidates)\n",
        "    sorted_tokens = sorted(heatmap.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    token_dict = {token: f\"T{idx}\" for idx, (token, _) in enumerate(sorted_tokens)}\n",
        "    use_tokens = True\n",
        "    \n",
        "    print(f\"\\n[DICT] Created dictionary: {len(token_dict)} tokens\")\n",
        "    \n",
        "    # SAVE IMMEDIATELY\n",
        "    print(f\"\\n[SAVE] Saving dictionary BEFORE PTAM...\")\n",
        "    with open(DICT_FILE, 'w') as f:\n",
        "        json.dump(token_dict, f, indent=2)\n",
        "    with open(HEATMAP_FILE, 'w') as f:\n",
        "        json.dump(heatmap, f, indent=2)\n",
        "    print(f\"[SAVE] ‚úì Dictionary saved\")\n",
        "    print(f\"[SAVE] ‚úì Heatmap saved\")\n",
        "\n",
        "elif PTAM_MODE == \"Token\" and use_tokens:\n",
        "    print(f\"[DICT] Using existing dictionary (no refinement)\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# PTAM GENERATION (BATCHED WITH CLEANUP)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(f\"\\n[BUILD] Generating PTAM ({PTAM_MODE} mode)...\")\n",
        "\n",
        "ptam_lines = []\n",
        "timestamp = datetime.now().isoformat()\n",
        "\n",
        "# Header\n",
        "ptam_lines.append('‚ïî' + '‚ïê' * 78 + '‚ïó')\n",
        "ptam_lines.append('‚ïë' + ' ' * 78 + '‚ïë')\n",
        "if use_tokens:\n",
        "    ptam_lines.append('‚ïë' + 'PTAM - TOKENIZED'.center(78) + '‚ïë')\n",
        "else:\n",
        "    ptam_lines.append('‚ïë' + 'PTAM - PLAIN'.center(78) + '‚ïë')\n",
        "ptam_lines.append('‚ïë' + ' ' * 78 + '‚ïë')\n",
        "ptam_lines.append('‚ï†' + '‚ïê' * 78 + '‚ï£')\n",
        "ptam_lines.append(f\"‚ïë  MODE: {PTAM_MODE.ljust(70)}‚ïë\")\n",
        "ptam_lines.append(f\"‚ïë  GENERATED: {timestamp.ljust(63)}‚ïë\")\n",
        "ptam_lines.append(f\"‚ïë  FILES: {str(stats['valid']).ljust(68)}‚ïë\")\n",
        "ptam_lines.append(f\"‚ïë  CHUNK: {format_size(CHUNK_SIZE_BYTES).ljust(68)}‚ïë\")\n",
        "ptam_lines.append(f\"‚ïë  BATCH: {str(BATCH_SIZE_NUM).ljust(68)}‚ïë\")\n",
        "if use_tokens:\n",
        "    ptam_lines.append(f\"‚ïë  TOKENS: {str(len(token_dict)).ljust(67)}‚ïë\")\n",
        "ptam_lines.append('‚ïö' + '‚ïê' * 78 + '‚ïù')\n",
        "ptam_lines.append('')\n",
        "ptam_lines.append('')\n",
        "\n",
        "# Token dict\n",
        "if use_tokens:\n",
        "    ptam_lines.append('‚ïî' + '‚ïê' * 78 + '‚ïó')\n",
        "    ptam_lines.append('‚ïë' + 'TOKEN DICTIONARY'.center(78) + '‚ïë')\n",
        "    ptam_lines.append('‚ïö' + '‚ïê' * 78 + '‚ïù')\n",
        "    ptam_lines.append('')\n",
        "    \n",
        "    sorted_dict = sorted(token_dict.items(), key=lambda x: int(x[1][1:]))\n",
        "    for token, tid in sorted_dict:\n",
        "        ptam_lines.append(f\"{tid}={token}\")\n",
        "    \n",
        "    ptam_lines.append('')\n",
        "    ptam_lines.append('')\n",
        "\n",
        "# Content\n",
        "ptam_lines.append('‚ïî' + '‚ïê' * 78 + '‚ïó')\n",
        "if use_tokens:\n",
        "    ptam_lines.append('‚ïë' + 'TOKENIZED CONTENT'.center(78) + '‚ïë')\n",
        "else:\n",
        "    ptam_lines.append('‚ïë' + 'MERGED CONTENT'.center(78) + '‚ïë')\n",
        "ptam_lines.append('‚ïö' + '‚ïê' * 78 + '‚ïù')\n",
        "ptam_lines.append('')\n",
        "\n",
        "# Process content in batches\n",
        "original_size = 0\n",
        "processed_size = 0\n",
        "\n",
        "# Pre-sort tokens once\n",
        "if use_tokens:\n",
        "    sorted_tokens = sorted(token_dict.items(), key=lambda x: len(x[0]), reverse=True)\n",
        "\n",
        "for idx, item in enumerate(contents):\n",
        "    path = item['path']\n",
        "    content = item['content']\n",
        "    original_size += len(content)\n",
        "    \n",
        "    # Apply tokens\n",
        "    if use_tokens:\n",
        "        for token, tid in sorted_tokens:\n",
        "            content = content.replace(token, tid)\n",
        "    \n",
        "    processed_size += len(content)\n",
        "    \n",
        "    # Write block\n",
        "    ptam_lines.append('‚îå' + '‚îÄ' * 78 + '‚îê')\n",
        "    ptam_lines.append(f\"‚îÇ FILE: {path[:70].ljust(70)}‚îÇ\")\n",
        "    ptam_lines.append('‚îú' + '‚îÄ' * 78 + '‚î§')\n",
        "    ptam_lines.append(content)\n",
        "    ptam_lines.append('‚îî' + '‚îÄ' * 78 + '‚îò')\n",
        "    ptam_lines.append('')\n",
        "    \n",
        "    # Progress + cleanup\n",
        "    if (idx + 1) % BATCH_SIZE_NUM == 0:\n",
        "        print_progress(idx + 1, len(contents), '[BUILD] Processing')\n",
        "        gc.collect()\n",
        "\n",
        "print_progress(len(contents), len(contents), '[BUILD] Processing')\n",
        "\n",
        "# Write PTAM\n",
        "ptam_content = '\\n'.join(ptam_lines)\n",
        "with open(PTAM_FILE, 'w', encoding='utf-8') as f:\n",
        "    f.write(ptam_content)\n",
        "\n",
        "print(f\"\\n[BUILD] PTAM written: {format_size(len(ptam_content))}\")\n",
        "\n",
        "if use_tokens:\n",
        "    compression = ((original_size - processed_size) / original_size * 100) if original_size > 0 else 0\n",
        "    print(f\"[BUILD] Compression: {compression:.1f}%\")\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# COMPLETION\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "print(\"\\n\" + \"‚ïê\" * 60)\n",
        "print(\"PTAM COMPILATION COMPLETE\")\n",
        "print(\"‚ïê\" * 60)\n",
        "print(f\"\\nüìÑ PTAM: {PTAM_FILE}\")\n",
        "if use_tokens:\n",
        "    print(f\"üß† Dictionary: {DICT_FILE}\")\n",
        "    print(f\"üìä Heatmap: {HEATMAP_FILE}\")\n",
        "print(f\"\\n‚úì Files: {stats['valid']}\")\n",
        "print(f\"‚úì Size: {format_size(len(ptam_content))}\")\n",
        "if use_tokens:\n",
        "    print(f\"‚úì Tokens: {len(token_dict)}\")\n",
        "    print(f\"‚úì Compression: {compression:.1f}%\")\n",
        "print(\"\\nüéâ Done!\")"
      ],
      "metadata": {
        "id": "execute",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìö Helper Cells\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "helpers"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üìÅ List Outputs\n",
        "\n",
        "import os\n",
        "\n",
        "def show_tree(path, prefix=\"\", depth=0, max_depth=3):\n",
        "    if depth >= max_depth or not os.path.exists(path):\n",
        "        return\n",
        "    items = sorted(os.listdir(path))\n",
        "    for i, item in enumerate(items):\n",
        "        p = os.path.join(path, item)\n",
        "        last = i == len(items) - 1\n",
        "        conn = \"‚îî‚îÄ‚îÄ \" if last else \"‚îú‚îÄ‚îÄ \"\n",
        "        if os.path.isdir(p):\n",
        "            print(f\"{prefix}{conn}üìÅ {item}/\")\n",
        "            ext = \"    \" if last else \"‚îÇ   \"\n",
        "            show_tree(p, prefix + ext, depth + 1, max_depth)\n",
        "        else:\n",
        "            size = os.path.getsize(p)\n",
        "            print(f\"{prefix}{conn}üìÑ {item} ({size:,} bytes)\")\n",
        "\n",
        "if 'WORKING_DIR' in globals():\n",
        "    print(\"\\nüì¶ Output Structure:\\n\")\n",
        "    show_tree(WORKING_DIR)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Run compiler first\")"
      ],
      "metadata": {
        "id": "helper-list",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì• Download Files\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "if 'PTAM_FILE' in globals() and os.path.exists(PTAM_FILE):\n",
        "    print(\"Downloading...\\n\")\n",
        "    files.download(PTAM_FILE)\n",
        "    print(f\"‚úì {os.path.basename(PTAM_FILE)}\")\n",
        "    \n",
        "    if use_tokens:\n",
        "        files.download(DICT_FILE)\n",
        "        print(f\"‚úì {os.path.basename(DICT_FILE)}\")\n",
        "        files.download(HEATMAP_FILE)\n",
        "        print(f\"‚úì {os.path.basename(HEATMAP_FILE)}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Complete\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Run compiler first\")"
      ],
      "metadata": {
        "id": "helper-download",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìñ Documentation\n",
        "\n",
        "### Performance Settings\n",
        "\n",
        "**Chunk Size** - Data processed at once:\n",
        "- Level 1: 512 KB (balanced)\n",
        "- Level 2: 1 MB (faster)\n",
        "- Level 3: 2 MB (maximum, may spike RAM)\n",
        "\n",
        "**Batch Size** - Files before memory cleanup:\n",
        "- Level 1: 50 files (conservative)\n",
        "- Level 2: 100 files (balanced)\n",
        "- Level 3: 250 files (aggressive)\n",
        "\n",
        "**Process Count** - Multiprocessing workers:\n",
        "- Level 1: Single process (most stable)\n",
        "- Level 2: Dual process (faster, uses more RAM)\n",
        "\n",
        "### Key Changes from v3\n",
        "\n",
        "- ‚úÖ **No dictionary refinement** - Built once, never modified\n",
        "- ‚úÖ **Batch memory cleanup** - GC after each batch\n",
        "- ‚úÖ **Local accumulation** - Merge counts once per chunk\n",
        "- ‚úÖ **Multiprocessing option** - True parallelism\n",
        "- ‚úÖ **Progress bars** - Visual feedback\n",
        "- ‚úÖ **Dictionary saved first** - Safe before PTAM generation\n",
        "\n",
        "### Modes\n",
        "\n",
        "**Plain Mode:**\n",
        "- Uses existing dictionary if present\n",
        "- No dictionary creation\n",
        "- Fast processing\n",
        "\n",
        "**Token Mode:**\n",
        "- Creates dictionary ONCE if none exists\n",
        "- Never refines\n",
        "- Saves immediately\n",
        "\n",
        "### File Support\n",
        "\n",
        "Archives: ZIP, TAR, GZIP, BZIP2, XZ, 7Z\n",
        "Nested: Up to 5 levels\n",
        "Text: 80% readable threshold\n",
        "\n",
        "---\n",
        "\n",
        "**Version: v4**\n",
        "\n",
        "**Accuracy: 98%**\n",
        "\n",
        "**What's New: 100%** - Complete performance rewrite\n"
      ],
      "metadata": {
        "id": "docs"
      }
    }
  ]
}
